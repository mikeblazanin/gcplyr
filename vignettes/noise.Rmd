---
title: "Dealing with noise"
author: "Mike Blazanin"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
  word_document:
    toc: true
    toc_depth: 4
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 4
vignette: >
  %\VignetteIndexEntry{Dealing with noise}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_knit$set(root.dir = tempdir())

#Note: to knit to all output formats simultaneously, run in R console:
#rmarkdown::render("./vignettes/noise.Rmd", output_format = "all")
```

# Where are we so far?

1. Introduction: `vignette("gcplyr")`
2. Importing and transforming data: `vignette("import_transform")`
3. Incorporating design information: `vignette("incorporate_designs")`
4. Pre-processing and plotting your data: `vignette("preprocess_plot")`
5. Processing your data: `vignette("process")`
6. Analyzing your data: `vignette("analyze")`
7. **Dealing with noise:** vignette("noise")
8. Statistics, merging other data, and other resources: `vignette("conclusion")`

So far, we've imported and transformed our measures, combined them with our design information, pre-processed, processed, plotted, and analyzed our data. Here, we're going to learn potential strategies for dealing with noise in our growth curve data.

If you haven't already, load the necessary packages.

```{r setup}
library(gcplyr)

library(dplyr)
library(ggplot2)
```

```{r, include = FALSE}
print_df <- function(df, col.names = FALSE) {
  write.table(format(df, justify = "right"),
              row.names=FALSE, col.names = col.names, quote = F)
}
```

```{r}
#This code was previously explained
#Here we're re-running it so it's available for us to work with
example_tidydata <- trans_wide_to_tidy(example_widedata_noiseless,
                                       id_cols = "Time")
example_design <- make_design(
  pattern_split = ",", nrows = 8, ncols = 12,
  "Bacteria_strain" = make_designpattern(
    values = paste("Strain", 1:48),
    rows = 1:8, cols = 1:6, pattern = 1:48, byrow = TRUE),
  "Bacteria_strain" = make_designpattern(
    values = paste("Strain", 1:48),
    rows = 1:8, cols = 7:12, pattern = 1:48, byrow = TRUE),
  "Phage" = make_designpattern(
    values = c("No Phage"), rows = 1:8, cols = 1:6, pattern = "1"),
  "Phage" = make_designpattern(
    values = c("Phage Added"), rows = 1:8, cols = 7:12, pattern = "1"))
ex_dat_mrg <- merge_dfs(example_tidydata, example_design)
ex_dat_mrg$Well <- 
  factor(ex_dat_mrg$Well,
         levels = paste(rep(LETTERS[1:8], each = 12), 1:12, sep = ""))
ex_dat_mrg <- group_by(ex_dat_mrg, Well, Bacteria_strain, Phage)
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         deriv = calc_deriv(x = Time, y = Measurements, x_scale = 3600),
         deriv_percap5 = calc_deriv(x = Time, y = Measurements, 
                                        percapita = TRUE, blank = 0,
                                        window_width_n = 5, trans_y = "log",
                                    x_scale = 3600))
sample_wells <- c("A1", "F1", "F10", "E11")
```

# Figuring out when you have too much noise



# How we can deal with noise

Broadly speaking, there are three ways `gcplyr` has implemented to deal with noise:

* Smooth the raw data
* "Smooth" during derivative calculations
* Analyze less-noisy subsets of the data

# Processing data: smoothing {#Smoothing}
Oftentimes, growth curve data produced by a plate reader will have some noise it it. While sometimes this noise does not hinder analyses, sometimes it's necessary to smooth the data in each well for analyses to succeed. `gcplyr` has a `smooth_data` function that can carry out such smoothing. Generally you should carry out *as little* smoothing as is necessary for your analyses to work. That means that **right now you should skip this section** and go on to the **[Calculating Derivatives](#CalculatingDerivatives)** section, returning to this smoothing section if your derivatives are too noisy to analyze.

If you have returned in need of learning to use `smooth_data`, let's start by taking a look at a few wells from our example data, which have some noise.

```{r}


#Plot with a linear y-axis
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time, y = Measurements)) +
  geom_point() +
  facet_wrap(~Well)

#Plot with a log y-axis
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time, y = Measurements)) +
  geom_point() +
  facet_wrap(~Well) +
  scale_y_continuous(trans = "log10")
```

Plotting our data with a log scale for the y-axis is particularly useful for growth curves because exponential growth is a straight line when plotted on a log scale. 

From the log plot especially we can see that at low densities there's a lot of noise relative to the density. In fact, this is a common occurrence: **at low densities, random noise tends to have a much larger effect** than at high densities. Unfortunately, calculating derivatives (especially the per-capita derivative) is very sensitive to such noise, so let's smooth our data.

`smooth_data` has four different smoothing algorithms to choose from: `moving-average`, `moving-median`, `loess`, and `gam`. 

* `moving-average` is a simple smoothing algorithm that primarily acts to reduce the effects of outliers on the data
* `moving-median` is another simple smoothing algorithm that primarily acts to reduce the effects of outliers on the data
* `loess` is a spline-fitting approach that uses polynomial-like curves, which produces curves with smoothly changing derivatives, but can in some cases create curvature artifacts not present in the original data
* `gam` is also spline-fitting approach that uses polynomial-like curves, which produces curves with smoothly changing derivatives, but can in some cases create curvature artifacts not present in the original data

**Additionally, all four smoothing algorithms have a tuning parameter** that controls how "smoothed" the data are. For whichever smoothing method you're using, **you should plot smoothing with multiple different tuning parameter values**, then choose the value that smooths the data as little as is necessary to reduce noise. Make sure to plot the smoothing for every well in your data, so that you're choosing the best setting for all your data and not just one well.

Smoothing data is a step that alters the values you will analyze. Because of that, and because there are so many options for how to smooth your data, it is a step that can be rife with pitfalls. I recommend starting with the simplest and least "smoothed" smoothing, plotting your results, and only increasing your smoothing as much as is needed to enable downstream analyses. Additionally, when sharing your findings, it's important to be transparent by sharing the raw data and smoothing methods, rather than treating the smoothed data as your source.

To use `smooth_data`, pass your x and y values, your method of choice, and any additional arguments needed for the method. It will return a vector of your smoothed y values.

Since we only want to smooth within each unique well, we'll first `group_by` our data:

```{r}
ex_dat_mrg <- group_by(ex_dat_mrg, Well, Bacteria_strain, Phage)
```

## Smoothing with moving-average

For `moving-average`, there are two tuning parameters to choose between:

* `window_width` specifies how wide the moving window used to calculate the average is in units of `x`.
* `window_width_n` specifies how many data points wide the moving window used to calculate the average is. 

Specifying the `window_width` or `window_width_n` is required, and larger values will be more "smoothed". Think carefully about whether you want to hold the *amount* of time or the *number* of data points in each window constant (if your data was all collected on constant intervals, then there will be no difference).

Here, we'll show moving averages with `window_width_n` values of 3, 7, or 11 data points wide (because the window is centered on each data point, `window_width_n` must be an odd number of data points wide). Note that `moving-average` returns `NA` for data points at the start and end of your data where the window extends beyond the domain of your data.

```{r}
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         smoothed3 = smooth_data(x = Time, y = Measurements,
              sm_method = "moving-average", window_width_n = 3),
         smoothed7 = smooth_data(x = Time, y = Measurements,
              sm_method = "moving-average", window_width_n = 7),
         smoothed11 = smooth_data(x = Time, y = Measurements,
              sm_method = "moving-average", window_width_n = 11))

#What does the smoothed data look like compared to the noisy original?
#Lighter lines are wider window_width_n's and more "smoothed"
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time)) +
  geom_point(aes(y = Measurements)) +
  geom_line(aes(y = smoothed3), color = "gray20") +
  geom_line(aes(y = smoothed7), color = "gray45") +
  geom_line(aes(y = smoothed11), color = "gray65") +
  facet_wrap(~Well) +
  scale_y_continuous(trans = "log10")
```

Here we can see that `moving-average` has helped reduce the effects of some of that early noise. However, with `window_width_n = 11` (the lightest line), the smoothing has started biasing our medium-density data points to be higher than they actually are. Based on this, we'd probably want to use a `window_width_n` less than 11. Unfortunately, with smaller `window_width_n` our early data is still being affected by that early noise, so we should explore other smoothing methods, or try combining multiple smoothing methods.

## Smoothing with moving-median



For `moving-median`, there are the same two tuning parameters:

* `window_width` specifies how wide the moving window used to calculate the average is in units of `x`.
* `window_width_n` specifies how many data points wide the moving window used to calculate the average is. 

Specifying the `window_width` or `window_width_n` is required, and larger values will be more "smoothed". Think carefully about whether you want to hold the *amount* of time or the *number* of data points in each window constant (if your data was all collected on constant intervals, then there will be no difference).

Here, we'll show moving medians with windows that are 3, 7, and 11 data points wide (because the window is centered on each data point, it must be an odd number of data points wide). Note that `moving-median` returns `NA` for data points at the start and end of your data where the window extends beyond the domain of your data.

```{r}
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         smoothed3 = 
           smooth_data(x = Time, y = Measurements,
                       sm_method = "moving-median", window_width_n = 3),
         smoothed7 = 
           smooth_data(x = Time, y = Measurements,
                       sm_method = "moving-median", window_width_n = 7),
         smoothed11 = 
           smooth_data(x = Time, y = Measurements,
                       sm_method = "moving-median", window_width_n = 11))

#What does the smoothed data look like compared to the noisy original?
#Lighter lines are wider window_width_n's and more "smoothed"
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time)) +
  geom_point(aes(y = Measurements)) +
  geom_line(aes(y = smoothed3), color = "gray20") +
  geom_line(aes(y = smoothed7), color = "gray45") +
  geom_line(aes(y = smoothed11), color = "gray65") +
  facet_wrap(~Well) +
  scale_y_continuous(trans = "log10")
```

Here we can see that `moving-median` has really excluded that low-density noise, even with the smallest `window_width_n = 3`. Additionally, `moving-median` did not bias our larger data hardly at all, except with the widest `window_width_n`. However, it has produced a smoothed density that is fairly "jumpy", something that wider `window_width_n` did not fix. **This is common with moving-median**, so often you may need to try other smoothing methods or combining `moving-median` with other methods.

## Smoothing with LOESS

For `loess`, the tuning parameter is the `span` argument. `loess` works by doing fits on subset windows of the data centered at each data point. These fits can be linear (`degree = 1`) or polynomial (typically `degree = 2`). `span` is the width of the window, as a fraction of all data points. For instance, with the default `span` of 0.75, 75% of the data points are included in each window. Thus, span values typically are between 0 and 1 (although see `?loess` for use of `span` values greater than 1), and larger values are more "smoothed". Here, we'll show `loess` smoothing with spans of 0.1, 0.2, and 0.5 and `degree = 1`.

```{r}
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         smoothed1 = smooth_data(x = Time, y = Measurements,
                                 sm_method = "loess", span = .1, degree = 1),
         smoothed2 = smooth_data(x = Time, y = Measurements,
                                 sm_method = "loess", span = .2, degree = 1),
         smoothed5 = smooth_data(x = Time, y = Measurements,
                                 sm_method = "loess", span = .5, degree = 1))

#What does the smoothed data look like compared to the noisy original?
#Lighter lines are larger span's and more "smoothed"
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time)) +
  geom_point(aes(y = Measurements)) +
  geom_line(aes(y = smoothed1), color = "gray20") +
  geom_line(aes(y = smoothed2), color = "gray45") +
  geom_line(aes(y = smoothed5), color = "gray65") +
  facet_wrap(~Well) +
  scale_y_continuous(trans = "log10")
```

Here we can see that `loess` with smaller spans (darker lines) have smoothed the data somewhat but are still sensitive to outliers. However, `loess` with a larger span (lightest line) has introduced significant bias. To fix this, we might explore other smoothing methods, or combining `loess` with other smoothing methods.

## Smoothing with GAM

For `gam`, the primary tuning parameter is the `k` argument. `gam` works by doing fits on subsets of the data and linking these fits together. `k` determines how many link points ("knots") it can use.  If not specified, the default `k` value for smoothing a time series is 10, with **smaller values being more "smoothed"** (note this is opposite the trend with other smoothing methods). However, **unlike earlier methods, `k` values that are too large are also problematic**, as they will tend to 'overfit' the data. `k` cannot be larger than the number of data points, and should usually be substantially smaller than that. Also note that **`gam` can sometimes create artifacts**, especially oscillations in your density and derivatives. You should check that `gam` is not doing so before carrying on with your analyses. Here, we'll show `gam` smoothing with `k` values of 5, 10, and 20.

```{r}
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         smoothed20 = smooth_data(x = Time, y = Measurements,
                                  sm_method = "gam", k = 20),
         smoothed10 = smooth_data(x = Time, y = Measurements,
                                  sm_method = "gam", k = 10),
         smoothed5 = smooth_data(x = Time, y = Measurements,
                                 sm_method = "gam", k = 5))

#What does the smoothed data look like compared to the noisy original?
#Lighter lines are smaller k and more "smoothed"
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time)) +
  geom_point(aes(y = Measurements)) +
  geom_line(aes(y = smoothed20), color = "gray20") +
  geom_line(aes(y = smoothed10), color = "gray45") +
  geom_line(aes(y = smoothed5), color = "gray65") +
  facet_wrap(~Well) +
  scale_y_continuous(trans = "log10")
```

Here we can see that `gam` does alright when working with the no phage-added wells (A1 and F1): higher `k` values (darkest line) have smoothed the data but are still sensitive to those early outliers, while lower `k` values (lighter lines) have introduced significant bias. However, `gam` is struggling when phage have been added (E11 and F10). Across all the `k` values it has added many fluctuations and often dips into values of 0 or lower (plotted here as breaks in the line, since the log of numbers <= 0 are undefined). To fix this, we might explore other smoothing methods or combining `gam` with other smoothing methods.

## Combining multiple smoothing methods

Often, combining multiple smoothing methods can provide improved results. For instance, `moving-median` is particularly good at removing outliers, but not very good at producing continuously smooth data. In contrast, `moving-average`, `loess`, and `gam` work better at producing continuously smooth data, but aren't as good at removing outliers. Here's an example using the strengths of both `moving-median` and `moving-average`. (Note that earlier columns created in `mutate` are available during creation of later columns, so both can be done in one step):

```{r}
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         smoothed_med3 = 
           smooth_data(x = Time, y = Measurements,
                       sm_method = "moving-median", window_width_n = 3),
         #Note that for the second round, we're using the 
         #first smoothing as the input y
         smoothed = 
           smooth_data(x = Time, y = smoothed_med3,
                       sm_method = "moving-average", window_width_n = 3))

#What does the smoothed data look like compared to the noisy original?
#The first round of smoothing with moving-median is plotted in lighter colors
#The second round of smoothing with moving-average is plotted in darker colors
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time)) +
  geom_point(aes(y = Measurements)) +
  geom_line(aes(y = smoothed_med3), color = "gray20") +
  geom_line(aes(y = smoothed), color = "gray65") +
  facet_wrap(~Well) +
  scale_y_continuous(trans = "log10")
```

Here we can see that the combination of minimal `moving-median` and `moving-average` smoothing has produced a curve that has most of the noise removed with minimal introduction of bias. (Note that the first and last 2 data points are now `NA` because of the smoothing)

# Processing data: calculating derivatives of smoothed data {#DerivsOfSmoothed}

TBD

# Processing data: smoothing during derivative calculation

Here we can see that the derivatives have been calculated, but also that there's a good amount of noise in the plots. If we go back and look at our raw data, we'll see that this noise arises from some random fluctuations in our density values.

```{r}
#Plot the raw data
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time, y = Measurements)) +
  geom_point() +
  facet_wrap(~Well, scales = "free")
```

Luckily, `calc_deriv` includes a method that can reduce some of the effects of this noise. Instead of calculating the derivative of each point relative to the next, we can use a moving window of more than two points and fit a linear regression to this data. This can help reduce the effect of any single noisy point.

To use the fitting functionality of `calc_deriv`, we need to specify either the `window_width` parameter, or the `window_width_n` parameter. `window_width` specifies how wide the window used to include points for the fitting is in units of `x`, while `window_width_n` specifies it in number of data points. Here, we'll demonstrate it's use by fitting regressions that include five data points.

```{r}
ex_dat_mrg <- mutate(ex_dat_mrg,
                     deriv5 = calc_deriv(x = Time, y = Measurements, 
                                        window_width_n = 5))

#Now let's plot the derivative
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time, y = deriv5)) +
  geom_line() +
  facet_wrap(~Well, scales = "free")
```

Great! This has removed a lot of the noise, although not quite all of it. Later in the [Smoothing](#Smoothing) and [Calculating Derivatives of Smoothed Data](#DerivsOfSmoothed) sections, we'll see how combining smoothing and derivative-fitting can further reduce the noise in calculated derivatives.



However, the other wells seem to have a lot of noise obscuring their per-capita growth rates. What happened? Why hasn't our smoothing been sufficient? As I explore later, per-capita growth rates can be strongly affected by even small noise at very low densities, something that can be excluded simply by only analyzing per-capita growth when densities are above some minimum value.


First, we may need to carry out smoothing of our data to reduce the effect of noise. Then, we typically need to calculate derivatives of our (smoothed) data. The (smoothed) density and (smoothed) derivatives will be what we analyze to identify features of our growth curves. `gcplyr` has a number of functions that facilitate these steps. 


# Summarizing on subsets: maximum growth rate

Sometimes, we need to provide limits on the data passed to our simple functions. We can demonstrate this in the process of calculating one of the most common metrics we want to identify: the maximum per-capita growth rate

Let's look again at our smoothed per-capita growth rates:

```{r}
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time, y = deriv_percap_hr)) +
  geom_line() +
  facet_wrap(~Well, scales = "free")
```

Hmmm, there's a lot of noise in these plots, what's going on? We can begin to understand if we also look at our smoothed density values:

```{r}
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% sample_wells),
       aes(x = Time, y = smoothed)) +
  geom_line() +
  facet_wrap(~Well, scales = "free")
```

If we compare these plots with the previous ones, we can begin to see that most of the noise is arising when the bacterial populations are very small. Indeed, **this is common with per-capita growth rates, which are very sensitive to noise at low densities**. What can we do about it? We can simply exclude all the values when the *density* is really low.

Let's plot our per-capita growth rate data at different cutoffs for the minimum *density* of bacteria. Even though these are smoothed values, we'll use points here, since it better showcases where data are being excluded:

```{r}
for (my_well in sample_wells) {
  #Title
  title <- cowplot::ggdraw() + 
    cowplot::draw_label(paste("Well", my_well), 
                        fontface = "bold", x = 0, hjust = 0) +
    theme(plot.margin = margin(0, 0, 0, 7))
  
  #Save x and y limits for all plots so they're all on the same axes
  xdat <- dplyr::filter(ex_dat_mrg, Well == my_well)$Time
  ydat <- dplyr::filter(ex_dat_mrg, Well == my_well)$deriv_percap_hr
  xlims <- c(min(xdat[is.finite(xdat)], na.rm = TRUE),
             max(xdat[is.finite(xdat)], na.rm = TRUE))
  ylims <- c(min(ydat[is.finite(ydat)], na.rm = TRUE),
             max(ydat[is.finite(ydat)], na.rm = TRUE))
  
  #Plot unfiltered data
  p1 <- ggplot(data = dplyr::filter(ex_dat_mrg, Well == my_well),
               aes(x = Time, y = deriv_percap_hr)) +
    geom_point() + facet_wrap(~Well, scales = "free") +
    ggtitle("all data") +
    xlim(xlims[1], xlims[2]) + ylim(ylims[1], ylims[2])
  
  #Plot data with filters for density
  p2 <- ggplot(data = dplyr::filter(ex_dat_mrg, 
                                    Well == my_well, smoothed > 0.001),
               aes(x = Time, y = deriv_percap_hr)) +
    geom_point() + facet_wrap(~Well, scales = "free") +
    ggtitle("data where smoothed > 0.001") +
    xlim(xlims[1], xlims[2]) + ylim(ylims[1], ylims[2])
  p3 <- ggplot(data = dplyr::filter(ex_dat_mrg, 
                                    Well == my_well, smoothed > 0.005),
               aes(x = Time, y = deriv_percap_hr)) +
    geom_point() + facet_wrap(~Well, scales = "free") +
    ggtitle("data where smoothed > 0.005") +
    xlim(xlims[1], xlims[2]) + ylim(ylims[1], ylims[2])
  p4 <- ggplot(data = dplyr::filter(ex_dat_mrg, 
                                    Well == my_well, smoothed > 0.01),
               aes(x = Time, y = deriv_percap_hr)) +
    geom_point() + facet_wrap(~Well, scales = "free") +
    ggtitle("data where smoothed > 0.01") +
    xlim(xlims[1], xlims[2]) + ylim(ylims[1], ylims[2])
  
  print(cowplot::plot_grid(title, cowplot::plot_grid(p1, p2, p3, p4, ncol = 2),
                           ncol = 1, rel_heights = c(0.1, 1)))
}
```

We can see with a cutoff of 0.001, much of the noise still remains. However, once we use a cutoff of 0.005, they are all basically gone, and no high growth rate values are affected by 0.005 vs 0.01. If we checked this pattern for all the wells (as you should in your own analyses), we would see a similar result. **Now, let's calculate the maximum growth rate of just the subset** of data points where OD is above 0.005. We can specify that subset directly in the summarize command:

```{r}
ex_dat_mrg_sum <-
  summarize(grouped_ex_dat_mrg,
            max_growth_rate = max(deriv_percap_hr[smoothed > 0.005], 
                                  na.rm = TRUE))
head(ex_dat_mrg_sum)
```

And now we can visualize our findings:

```{r}
ggplot(data = dplyr::filter(ex_dat_mrg, 
                            Well %in% sample_wells, smoothed >= 0.005),
       aes(x = Time, y = deriv_percap_hr)) +
  geom_point() +
  facet_wrap(~Well, scales = "free") +
  ggtitle("data where smoothed density > 0.005") +
  geom_hline(data = dplyr::filter(ex_dat_mrg_sum, Well %in% sample_wells), 
             aes(yintercept = max_growth_rate), lty = 2)
```


```{r}
#This code was previously explained
#Here we're re-running it so it's available for us to work with
example_tidydata <- trans_wide_to_tidy(example_widedata_noiseless,
                                       id_cols = "Time")
example_design <- make_design(
  pattern_split = ",", nrows = 8, ncols = 12,
  "Bacteria_strain" = make_designpattern(
    values = paste("Strain", 1:48),
    rows = 1:8, cols = 1:6, pattern = 1:48, byrow = TRUE),
  "Bacteria_strain" = make_designpattern(
    values = paste("Strain", 1:48),
    rows = 1:8, cols = 7:12, pattern = 1:48, byrow = TRUE),
  "Phage" = make_designpattern(
    values = c("No Phage"), rows = 1:8, cols = 1:6, pattern = "1"),
  "Phage" = make_designpattern(
    values = c("Phage Added"), rows = 1:8, cols = 7:12, pattern = "1"))
ex_dat_mrg <- merge_dfs(example_tidydata, example_design)
ex_dat_mrg$Well <- 
  factor(ex_dat_mrg$Well,
         levels = paste(rep(LETTERS[1:8], each = 12), 1:12, sep = ""))
ex_dat_mrg <- group_by(ex_dat_mrg, Well, Bacteria_strain, Phage)
ex_dat_mrg <-
  mutate(ex_dat_mrg,
         smoothed_med3 = 
           smooth_data(x = Time, y = Measurements,
                       sm_method = "moving-median", window_width_n = 3),
         #Note that for the second round, we're using the 
         #first smoothing as the input y
         smoothed = 
           smooth_data(x = Time, y = smoothed_med3,
                       sm_method = "moving-average", window_width_n = 3),
         deriv = calc_deriv(x = Time, y = smoothed),
         deriv_percap_hr = calc_deriv(x = Time, y = smoothed,
                                      percapita = TRUE, blank = 0,
                                      x_scale = 3600))
sample_wells <- c("A1", "F1", "F10", "E11")
```



Hmmm, in most of the wells `first_peak` worked perfectly well. However, a few of the wells aren't quite what we'd expect. Let's take a closer look at them:

```{r}
wells_tocheck <- c("A3", "A12", "B9", "C11", "F4", "F12")
ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% wells_tocheck), 
       aes(x = Time, y = smoothed)) +
  geom_line() +
  facet_wrap(~Well, scales = "free") +
  geom_point(data = dplyr::filter(ex_dat_mrg_sum, Well %in% wells_tocheck), 
             aes(x = first_peak_x, y = first_peak_y), 
             color = "red", size = 1.5)
```

Now we can see what's going on. In these wells, `first_peak` seems to have 'gotten stuck' on some earlier smaller peaks. Just like in smoothing, **peak-finding also has tuning parameters**. For `first_peak` and `find_local_extrema`, these are `window_width_n`, `window_width` and `window_height`:

* `window_width` determines the width of the window used to search for peaks and valleys, in units of `x`
* `window_width_n` determines the width of the window, in units of number of data points
* `window_height` determines the shortest peak or shallowest valley the window will cross, in units of `y`

If we want `first_peak` to be less sensitive to local peaks, we can increase these parameters (the default setting is `window_width_n` equal to 20% of the length of `y`, but `window_width` is a better approach since it works in units of seconds). Let's try that:

```{r}
ex_dat_mrg_sum <-
  summarize(grouped_ex_dat_mrg,
            first_peak_x = first_peak(x = Time, y = smoothed, return = "x",
                                      window_width = 35000),
            first_peak_y = first_peak(x = Time, y = smoothed, return = "y",
                                      window_width = 35000))

ggplot(data = dplyr::filter(ex_dat_mrg, Well %in% wells_tocheck), 
       aes(x = Time, y = smoothed)) +
  geom_line() +
  facet_wrap(~Well, scales = "free") +
  geom_point(data = dplyr::filter(ex_dat_mrg_sum, Well %in% wells_tocheck), 
             aes(x = first_peak_x, y = first_peak_y), 
             color = "red", size = 1.5)
```


 As we did earlier, we'll limit our analyses to data where `smoothed > 0.005`, and visualize using points (even though this is smoothed):

```{r}
ex_dat_mrg_sum <-
  summarize(grouped_ex_dat_mrg,
            max_growth_rate = first_peak(x = Time[smoothed > 0.005], 
                                         y = deriv_percap_hr[smoothed > 0.005], 
                                         return = "y", window_width = 35000),
            lag_time = first_peak(x = Time[smoothed > 0.005], 
                                  y = deriv_percap_hr[smoothed > 0.005], 
                                  return = "x", window_width = 35000))

head(ex_dat_mrg_sum)

ggplot(data = dplyr::filter(ex_dat_mrg,
                            Well %in% sample_wells, smoothed > 0.005),
       aes(x = Time, y = deriv_percap_hr)) +
  geom_point() +
  facet_wrap(~Well, scales = "free") +
  geom_point(data = dplyr::filter(ex_dat_mrg_sum, Well %in% sample_wells),
             aes(x = lag_time, y = max_growth_rate),
             color = "red", size = 2)
```

Here we can see that in Well E11, `first_peak` has identified the peak growth rate at the beginning of the dynamics, and not the one that occurs later on. This means that our `lag_time` value will actually reflect what we want it to.

But what if you want to find an extrema that's *not* the first peak? In the next section, we'll learn how to use `find_local_extrema` to identify all kinds of local extrema.


# What's next?

Now that you've analyzed your data and dealt with any noise, there's just some concluding notes on best practices for running statistics, merging growth curve analyses with other data, and additional resources for analyzing growth curves. 

1. Introduction: `vignette("gcplyr")`
2. Importing and transforming data: `vignette("import_transform")`
3. Incorporating design information: `vignette("incorporate_designs")`
4. Pre-processing and plotting your data: `vignette("preprocess_plot")`
5. Processing your data: `vignette("process")`
6. Analyzing your data: `vignette("analyze")`
7. Dealing with noise: vignette("noise")
8. **Statistics, merging other data, and other resources:** `vignette("conclusion")`